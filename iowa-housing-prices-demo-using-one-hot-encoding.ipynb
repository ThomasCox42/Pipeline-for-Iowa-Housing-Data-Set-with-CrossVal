{"cells":[{"metadata":{"trusted":true,"_uuid":"fa3dd3530b4f94ea453b514ff86118c4b700a0c9"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n# Loading in Iowa housing data\nmain_file_path = '../input/house-prices-advanced-regression-techniques/train.csv' \n# this is the path to the Iowa data that you will use\niowa_data = pd.read_csv(main_file_path)\nprint('Setup Complete...')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b928a78f93e219f84fdd64a841e95b40bf403cfd"},"cell_type":"code","source":"# import what we need for scikit to set up our model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\niowa_target = iowa_data.SalePrice\niowa_predictors = iowa_data.drop(['SalePrice'], axis=1)\n\n# we will only use numeric predictors for this model\niowa_numeric_predictors = iowa_predictors.select_dtypes(exclude=['object'])\nprint(iowa_numeric_predictors.columns)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70430d7afffa8f864ce8db6985a73dee23f8544d"},"cell_type":"markdown","source":"First we will need to split our data into training and testing sets, then we create a function to compare the quality of the different approaches we will take to get rid of our missing values."},{"metadata":{"_uuid":"05eed0c4bf66fc8badbf9cb1b5c12362fddb755a","trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(iowa_numeric_predictors, \n                                                    iowa_target,\n                                                    train_size=0.7, \n                                                    test_size=0.3, \n                                                    random_state=0)\n\ndef score_dataset(X_train, X_test, y_train, y_test):\n    model = RandomForestRegressor()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return mean_absolute_error(y_test, preds)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4544dce23d3b2790a8c88e359ac14a7b26842866"},"cell_type":"markdown","source":"First we will train a model that simply drops any columns with missing values and get its MAE"},{"metadata":{"trusted":true,"_uuid":"faf4e41da547969684991c340bc47208e2352e7f"},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning) #so pandas doesn't spit out a warning everytime\ncols_with_missing = [col for col in X_train.columns \n                                 if X_train[col].isnull().any()]\nreduced_X_train = X_train.drop(cols_with_missing, axis=1)\nreduced_X_test  = X_test.drop(cols_with_missing, axis=1)\nprint(\"Mean Absolute Error from dropping columns with Missing Values:\")\nprint(score_dataset(reduced_X_train, reduced_X_test, y_train, y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6bbf9ca6dab5923db40c3bdf9c1844f5dac84c40"},"cell_type":"markdown","source":"Now we will get the MAE for our model that uses Imputation instead"},{"metadata":{"_uuid":"d3a63f5dc468a795cc891905ec906835d09255f8","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import Imputer as Imputer\nmy_imputer = Imputer()\nimputed_X_train = my_imputer.fit_transform(X_train)\nimputed_X_test = my_imputer.transform(X_test)\nprint(\"Mean Absolute Error from Imputation:\")\nprint(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2715c89c7e62b33d11fc6c3d3e520765ab0174a8"},"cell_type":"markdown","source":"Now for fun we will get the model score for when we use imputation with extra columns showing"},{"metadata":{"trusted":true,"_uuid":"3ee8cba7bfec1a499f8ce4d7021943cbeef364a1"},"cell_type":"code","source":"imputed_X_train_plus = X_train.copy()\nimputed_X_test_plus = X_test.copy()\n\ncols_with_missing = (col for col in X_train.columns\n                                 if X_train[col].isnull().any())\nfor col in cols_with_missing:\n    imputed_X_train_plus[col + '_was_missing'] = imputed_X_train_plus[col].isnull()\n    imputed_X_test_plus[col + '_was_missing'] = imputed_X_test_plus[col].isnull()\n\n# Imputation\nmy_imputer = Imputer()\nimputed_X_train_plus = my_imputer.fit_transform(imputed_X_train_plus)\nimputed_X_test_plus = my_imputer.transform(imputed_X_test_plus)\n\nprint(\"Mean Absolute Error from Imputation while Track What Was Imputed:\")\nprint(score_dataset(imputed_X_train_plus, imputed_X_test_plus, y_train, y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"110987884cfd1cf8582139a66df60b75ee73af92"},"cell_type":"markdown","source":"# One-Hot Encoding\n**This MAE is better than our previous results, but we are still throwing out a bunch of data by only predicting our Sale price based on only our numeric data. To solve this and get more accurate results we will start over and implement One-Hot Encoding.**"},{"metadata":{"_uuid":"48c10bb1b1bfdbfe3aab274f41c7dbac7aaa0070","trusted":true},"cell_type":"code","source":"# Read the data\n# import pandas as pd #ALREADY IMPORTED ABOVE\n\ntrain_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest_data = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n\n# Drop houses where the target is missing\ntrain_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\ntarget = train_data.SalePrice\n\n# Since missing values isn't the focus of this tutorial, we use the simplest\n# possible approach, which drops these columns. \n# For more detail (and a better approach) to missing values, see\n# https://www.kaggle.com/dansbecker/handling-missing-values\ncols_with_missing = [col for col in train_data.columns \n                                 if train_data[col].isnull().any()]                                  \ncandidate_train_predictors = train_data.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\ncandidate_test_predictors = test_data.drop(['Id'] + cols_with_missing, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f98d3ee06e2ce858afe645725a93ecff34be8e07"},"cell_type":"markdown","source":"**\"Cardinality\" means the number of unique values in a column.\nWe use it as our only way to select categorical columns here. This is convenient, though a little arbitrary.**"},{"metadata":{"trusted":true,"_uuid":"47d50da6ee999bf6adcd2760b5031b295c589f96"},"cell_type":"code","source":"low_cardinality_cols = [cname for cname in candidate_train_predictors.columns if\n                       candidate_train_predictors[cname].nunique() < 10 and\n                       candidate_train_predictors[cname].dtype == 'object']\nnumeric_cols = [cname for cname in candidate_train_predictors.columns if \n                                candidate_train_predictors[cname].dtype in ['int64', 'float64']]\nmy_cols = low_cardinality_cols + numeric_cols\ntrain_predictors = candidate_train_predictors[my_cols]\ntest_predictors = candidate_train_predictors[my_cols]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc2ede1c23c36f9caa08936c08b11295b4fcaef0"},"cell_type":"markdown","source":"Pandas assigns a data type (called a dtype) to each column or Series. Let's see a random sample of dtypes from our prediction data:"},{"metadata":{"trusted":true,"_uuid":"6b57cbdcb66d2996a2b96d798ca97e149ce99ba2"},"cell_type":"code","source":"train_predictors.dtypes.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7085a3444e499a3e6c29eef0159f59bed159c1f9"},"cell_type":"markdown","source":"**Object** indicates a column has text (there are other things it could be theoretically be, but that's unimportant for our purposes). It's most common to one-hot encode these \"object\" columns, since they can't be plugged directly into most models. Pandas offers a convenient function called **get_dummies** to get one-hot encodings. Call it like this:"},{"metadata":{"trusted":true,"_uuid":"3d952c57a5d6a14b39d7fcd8270a9fc1efd73497"},"cell_type":"code","source":"one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"591271031264fe7c57968889af03f55c118465ca"},"cell_type":"markdown","source":"Alternatively, we can drop these object columns and compare the MAE of the two methods to see which works best for our dataset:\n\n1. One-hot encoded categoricals as well as numeric predictors\n\n2. Numerical predictors, where we drop categoricals.\n\n"},{"metadata":{"trusted":true,"_uuid":"abbe6a1191935e74636debc8347bd6e144413a79"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\n\ndef get_mae(X,y):\n    # we multiply by -1 in this instance in order to ouput a positve MAE score \n    # instead of a negative value returned by sklearn\n    return -1 * cross_val_score(RandomForestRegressor(50),\n                                X,y,\n                                scoring = 'neg_mean_absolute_error').mean()\n\npredictors_without_categoricals = train_predictors.select_dtypes(exclude=['object'])\n\nmae_without_categoricals = get_mae(predictors_without_categoricals, target)\n\nmae_one_hot_encoded = get_mae(one_hot_encoded_training_predictors, target)\n\nprint('Mean Absolute Error when Dropping Categoricals: ' + str(int(mae_without_categoricals)))\nprint('Mean Absolute Error with One-Hot Encoding: ' + str(int(mae_one_hot_encoded)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b53d447a3d2b2496ed09ff3cc0e211f4791351d"},"cell_type":"markdown","source":"# Applying to Multiple Files\nSo far, you've one-hot-encoded your training data. What about when you have multiple files (e.g. a test dataset, or some other data that you'd like to make predictions for)? Scikit-learn is sensitive to the ordering of columns, so if the training dataset and test datasets get misaligned, your results will be nonsense. This could happen if a categorical had a different number of values in the training data vs the test data.\n\n**Ensure the test data is encoded in the same manner as the training data with the align command:**"},{"metadata":{"trusted":true,"_uuid":"4f2996f56240babaeed3430d75ef2db99a574876"},"cell_type":"code","source":"one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\none_hot_encoded_test_predictors = pd.get_dummies(test_predictors)\nfinal_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n                                                                   \n                                                                   join='left',\n                                                                   \n                                                                   axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da9551fcedc782fd13cab27dbb82857762a1c4b4"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"46fca24ebf8d268bce7aa54d00fd9b9515aa77d3"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}